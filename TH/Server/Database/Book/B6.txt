***** Algorithmic Information Theory *****


1    Introduction
How should we measure the amount of information about a phenomenon that is given
to us by an observation concerning the phenomenon? Both classical (Shannon) in-
formation theory and algorithmic information theory start with the idea that this
amount can be measured by the mini-mum number of bits needed to describe the observation.
But whereas Shannon's theory considers description methods that are optimal relative to
some given probability distri-bution, any computer program that first computes (prints)
the string representing the observa-tion, and then terminates, is viewed as a valid 
description. The amount of information in the string is then defined as the size (measured
in bits) of the shortest computer program that outputs the string and then terminates.
    Such a definition would appear to make the amount of information in a string (or
other object) depend on the particular programming language used. Fortunately, it
can be shown that all reasonable choices of programming languages lead to quantifi-
cation of the amount of 'absolute' information in individual objects that is invariant
up to an additive constant. We call this quantity the 'Kolmogorov complexity' of the
object. While regular strings have small Kolmogorov complexity, random strings have
Kolmogorov complexity about equal to their own length. Measuring complexity and
information in terms of program size has turned out to be a very powerful idea with
applications in areas such as theoretical computer science, logic, probability theory,
statistics and physics.

This Chapter Kolmogorov complexity was introduced independently and with dif-
ferent motivations by R.J. Solomonoff (born 1926), A.N. Kolmogorov (1903-1987) and
G. Chaitin (born 1943) in 1960/1964, 1965 and 1966 respectively [Solomonoff 1964;
Kolmogorov 1965; Chaitin 1966]. During the last forty years, the subject has devel-
oped into a major and mature area of research. Here, we give a brief overview of the
subject geared towards an audience specifically interested in the philosophy of informa-
tion. With the exception of the recent work on the Kolmogorov structure function and
parts of the discussion on philosophical implications, all material we discuss here can
also be found in the standard textbook. The chapter is struc-
tured as follows: we start with an introductory section in which we define Kolmogorov
complexity and list its most important properties. We do this in a much simplified (yet
formally correct) manner, avoiding both technicalities and all questions of motivation
(why this definition and not another one?). This is followed by Section 3 which pro-
vides an informal overview of the more technical topics discussed later in this chapter,
in Sections 4- 6. The final Section 7, which discusses the theory's philosophical impli-
cations, as well as Section 6.3, which discusses the connection to inductive inference,
are less technical again, and should perhaps be glossed over before delving into the
technicalities of Sections 4 6.


2      Kolmogorov Complexity: Essentials
The aim of this section is to introduce our main notion in the fastest and simplest
possible manner, avoiding, to the extent that this is possible, all technical and motiva-
tional issues. Section 2.1 provides a simple definition of Kolmogorov complexity. We
list some of its key properties in Section 2.2. Knowledge of these key properties is an
essential prerequisite for understanding the advanced topics treated in later sections.